# instructlab-gpu.container
# Purpose: Run an InstructLab-ready GPU container on top of NVIDIA CUDA 12.6 + cuDNN on UBI9
# Image:    nvidia/cuda:12.6.0-cudnn-runtime-ubi9
# Persist:  Bind-mounts a host directory into the container (/workspace)
# Runtime:  Systemd-managed Podman Quadlet, restarted automatically on failure/boot

[Unit]
Description=InstructLab (GPU) - CUDA 12.6 cuDNN UBI9
# Start _after_ the user session is established (rootless) and network is up
After=network-online.target
Wants=network-online.target

[Container]
# --- Image selection ---
# Pull the recommended Instructlab image
Image=redhat/instructlab:latest

# --- Naming and lifecycle ---
# A stable container name helps with 'podman exec' and logs
ContainerName=instructlab-gpu



# --- GPU / NVIDIA integration ---
# These env vars are consumed by the NVIDIA OCI hook (provided by nvidia-container-toolkit)
# to inject the right libraries/devices. Make sure the toolkit is installed on the host.
Environment=NVIDIA_VISIBLE_DEVICES=all #Legacy settings 
Environment=NVIDIA_DRIVER_CAPABILITIES=compute,utility  #Legacy settings
AddDevice=nvidia.com/gpu=all

# -----------------------------
# SELinux options (Fedora/RHEL)
# -----------------------------
# Option A) Relaxed SELinux (default)
# - Simplest for GPU/CDI: disables the container SELinux label so /dev/nvidia* devices
#   and bind mounts work without extra labeling.
SecurityLabelDisable=true

# Option B) Strict SELinux (keep confinement)  <-- RECOMMENDED if you want strict policy
# - Comment OUT SecurityLabelDisable=true above
# - Add :z (shared) or :Z (private) to your bind mounts below
# - Ensure the host path is labeled and device use is allowed
#
#   Host prep (one-time):
#     sudo setsebool -P container_use_devices=true
#     sudo chcon -Rt container_file_t ~/instructlab-data
#
#   Volume lines with SELinux labels (uncomment one style):
# Volume=%h/instructlab-data:/workspace:z       # :z = shared label (multiple containers)
# Volume=%h/instructlab-data/.cache:/root/.cache:z
# Volume=%h/instructlab-data:/workspace:Z       # :Z = private label (isolated)
# Volume=%h/instructlab-data/.cache:/root/.cache:Z

# --- Workdir & persistence ---
# Choose a host path to persist data. For rootless, the deploy script defaults to:  ~/instructlab-data
# This bind mount is your "project root" inside the container.
# Add :z on SELinux hosts to label the path for container sharing (the deploy script will do this automatically).
Volume=%h/instructlab-data:/workspace:z

# Cache directory to persist pip/wheels/model caches across restarts (optional but recommended).
Volume=%h/instructlab-data/.cache:/root/.cache:z

# --- Networking ---
# Uncomment and adjust if you plan to expose any InstructLab service/ports later. This should be changed to a none standard port at somepoint in the future. 
# PublishPort=8080:8080/tcp

# --- User/Workdir ---
# Work inside /workspace so 'podman exec -it instructlab-gpu bash' lands you in the project.
WorkingDir=/workspace

# --- Entry command ---
# Keep the container "up" so you can exec into it anytime (you can swap this for a launcher script later).
# Using bash -lc allows profile scripts/conda etc. if you add them later.
Command=/bin/bash -lc "sleep infinity"

# --- Logging ---
# Keep logs accessible via 'journalctl --user -u instructlab-gpu.service' (rootless)
# and 'podman logs instructlab-gpu'.
# (Quadlet defaults are fineâ€”no extra stanza needed.)

# Give the container a little time to stop gracefully on shutdown/reload
TimeoutStopSec=30
# Ensures NVIDIA kernel modules create /dev/nvidia-uvm before container start
ExecStartPre=/bin/bash -c 'while true; do /usr/bin/nvidia-smi -L && exit 0; sleep 1; done'
Exec=/bin/sh -lc 'echo "InstructLab GPU container is up" && sleep infinity' # Just Added 

[Service]
# Always restart to survive crashes; systemd handles backoff
Restart=always
TimeoutStopSec=60

[Install]
# For rootless: enables on user login; for rootful: enables at system boot
WantedBy=default.target
